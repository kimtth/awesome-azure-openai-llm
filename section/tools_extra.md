# Tools, Datasets, and Evaluation

### **Contents**

- [General AI Tools and Extensions](#general-ai-tools-and-extensions)
- [LLM for Robotics](#llm-for-robotics)
- [Awesome Demo](#awesome-demo)
- [Datasets for LLM Training](#datasets-for-llm-training)
- [Evaluating Large Language Models](#evaluating-large-language-models)
- [LLMOps: Large Language Model Operations](#llmops-large-language-model-operations)

## **General AI Tools and Extensions**

- [5 LLM-based Apps for Developers](https://hackernoon.com/5-llm-based-apps-for-developers): Github Copilot, Cursor IDE, Tabnine, Warp, Replit Agent
- AI Search engine:
  - [Phind](https://www.phind.com/search): AI-Powered Search Engine for Developers [July 2022]
  - [Perplexity](http://perplexity.ai) [Dec 2022]
  - [Perplexity comet](https://www.perplexity.ai/comet): agentic browser [9 Jul 2025]
  - [GenSpark](https://www.genspark.ai/): AI agents engine perform research and generate custom pages called Sparkpages. [18 Jun 2024]
  - [felo.ai](https://felo.ai/search): Sparticle Inc. in Tokyo, Japan [04 Sep 2024]
  - [Goover](https://goover.ai/) 
  - [oo.ai](https://oo.ai): Open Research. Fastest AI Search.
- AI Tools: <https://aitoolmall.com/>
- [Ai2 Playground](https://playground.allenai.org/)
- Airtable list: [Generative AI Index](https://airtable.com/appssJes9NF1i5xCn/shrH4REIgddv8SzUo/tbl5dsXdD1P859QLO) | [AI Startups](https://airtable.com/appSpVXpylJxMZiWS/shr6nfE9FOHp17IjG/tblL3ekHZfkm3p6YT)
- [AlphaXiv](https://www.arxiv.org): an interactive extension of arXiv
- [AniDoc‚ú®](https://github.com/yihao-meng/AniDoc): Animation Creation Made Easier [‚úçÔ∏è](https://yihao-meng.github.io/AniDoc_demo/) ![**github stars**](https://img.shields.io/github/stars/yihao-meng/AniDoc?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
- [Cherry Studio‚ú®](https://github.com/CherryHQ/cherry-studio): a desktop client that supports multiple LLM providers. ![**github stars**](https://img.shields.io/github/stars/CherryHQ/cherry-studio?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
- Content writing: <http://jasper.ai/chat> / [üó£Ô∏è](https://twitter.com/slow_developer/status/1671530676045094915)
- [Duck.ai](https://www.duck.ai/):üí°Private, Useful, and Optional AI: DuckDuckGo offers free access to popular AI chatbots at Duck.ai
- Edge and Chrome Extension & Plugin
  - [MaxAI.me](https://www.maxai.me/)
  - [BetterChatGPT‚ú®](https://github.com/ztjhz/BetterChatGPT)
 ![**github stars**](https://img.shields.io/github/stars/ztjhz/BetterChatGPT?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
  - [ChatHub‚ú®](https://github.com/chathub-dev/chathub) All-in-one chatbot client [Webpage](https://chathub.gg/)
 ![**github stars**](https://img.shields.io/github/stars/chathub-dev/chathub?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
  - [ChatGPT Retrieval Plugin‚ú®](https://github.com/openai/chatgpt-retrieval-plugin)
 ![**github stars**](https://img.shields.io/github/stars/openai/chatgpt-retrieval-plugin?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
- [FLORA](https://www.florafauna.ai/): an AI platform integrating text, image, and video models into a unified canvas.
- Future Tools: <https://www.futuretools.io/>
- [God Tier Prompts](https://www.godtierprompts.com): A community driven leaderboard where the best prompts rise to the top.
- Open Source Image Creation Tool
  - ComfyUI - https://github.com/comfyanonymous/ComfyUI
  - Stable Diffusion web UI - https://github.com/AUTOMATIC1111/stable-diffusion-webui
- [INFP: Audio-Driven Interactive Head Generation in Dyadic Conversations](https://grisoon.github.io/INFP/) [refüìë](https://arxiv.org/abs/2412.04037) [5 Dec 2024]
- [MGX (MetaGPT X)](https://mgx.dev/): Multi-agent collaboration platform to develop an application.
- [Msty](https://msty.app/):üí°The easiest way to use local and online AI models
- [napkin.ai](https://www.napkin.ai/): a text-to-visual graphics generator [7 Aug 2024]
- Newsletters & Tool Databas: <https://www.therundown.ai/>
- Open Source No-Code AI Tools
  - Anything-LLM ‚Äî https://anythingllm.com
  - Budibase ‚Äî https://budibase.com
  - Coze Studio ‚Äî https://www.coze.com
  - Dify ‚Äî https://dify.ai
  - Flowise ‚Äî https://flowiseai.com
  - n8n ‚Äî https://n8n.io
  - NocoBase ‚Äî https://www.nocobase.com
  - NocoDB ‚Äî https://nocodb.com
  - Sim ‚Äî https://www.sim.ai
  - Strapi ‚Äî https://strapi.io
  - ToolJet ‚Äî https://www.tooljet.ai
- Oceans of AI - All AI Tools <https://play.google.com/store/apps/details?id=in.blueplanetapps.oceansofai&hl=en_US>
- Open source (huggingface):ü§ó<http://huggingface.co/chat>
- [Pika AI - Free AI Video Generator](https://pika.art/login)
- [Product Hunt > AI](https://www.producthunt.com/categories/ai)
- [Quora Poe](https://poe.com/login) A chatbot service that gives access to GPT-4, gpt-3.5-turbo, Claude from Anthropic, and a variety of other bots. [Feb 2023]
- [recraft.ai](https://www.recraft.ai/): Text-to-editable vector image generator
- [Same.dev](https://same.new/): Clone Any Website in Minutes
- [skywork.ai](https://skywork.ai): Deep Research is a multimodal generalist agent that can create documents, slides, and spreadsheets.
- [Smartsub](https://smartsub.ai/): AI-powered transcription, translation, and subtitle creation
- [TEXT-TO-CAD](https://zoo.dev/text-to-cad): Generate CAD from text prompts
- The leader: <http://openai.com>
- The runner-up: <http://bard.google.com> -> <https://gemini.google.com>
- Toolerific.ai: <https://toolerific.ai/>: Find the best AI tools for your tasks
- [Vercel AI](https://sdk.vercel.ai/) Vercel AI Playground / Vercel AI SDK [‚ú®](https://github.com/vercel/ai) [May 2023]
 ![**github stars**](https://img.shields.io/github/stars/vercel/ai?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
- [websim.ai](https://websim.ai/): a web editor and simulator that can generate websites. [1 Jul 2024]
- allAIstartups: <https://www.allaistartups.com/ai-tools>

## **LLM for Robotics**

- PromptCraft-Robotics: Robotics and a robot simulator with ChatGPT integration [‚ú®](https://github.com/microsoft/PromptCraft-Robotics) [Feb 2023]
 ![**github stars**](https://img.shields.io/github/stars/microsoft/PromptCraft-Robotics?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
- ChatGPT-Robot-Manipulation-Prompts: A set of prompts for Communication between humans and robots for executing tasks. [‚ú®](https://github.com/microsoft/ChatGPT-Robot-Manipulation-Prompts) [Apr 2023]
 ![**github stars**](https://img.shields.io/github/stars/microsoft/ChatGPT-Robot-Manipulation-Prompts?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
- Siemens Industrial Copilot [‚úçÔ∏è](https://news.microsoft.com/2023/10/31/siemens-and-microsoft-partner-to-drive-cross-industry-ai-adoption/)  [31 Oct 2023]
- [LeRobotü§ó](https://huggingface.co/lerobot): Hugging Face. LeRobot aims to provide models, datasets, and tools for real-world robotics in PyTorch. [‚ú®](https://github.com/huggingface/lerobot) [Jan 2024]
 ![**github stars**](https://img.shields.io/github/stars/huggingface/lerobot?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
- [Mobile ALOHA](https://mobile-aloha.github.io/): Stanford‚Äôs mobile ALOHA robot learns from humans to cook, clean, do laundry. Mobile ALOHA extends the original ALOHA system by mounting it on a wheeled base [‚úçÔ∏è](https://venturebeat.com/automation/stanfords-mobile-aloha-robot-learns-from-humans-to-cook-clean-do-laundry/) [4 Jan 2024] / [ALOHA](https://www.trossenrobotics.com/aloha.aspx): A Low-cost Open-source Hardware System for Bimanual Teleoperation.
- [Figure 01 + OpenAI](https://www.figure.ai/): Humanoid Robots Powered by OpenAI ChatGPT [üì∫](https://youtu.be/Sq1QZB5baNw?si=wyufZA1xtTYRfLf3) [Mar 2024]
- [Gemini Robotics](https://deepmind.google/discover/blog/gemini-robotics-brings-ai-into-the-physical-world/): Robotics built on the foundation of Gemini 2.0 [12 Mar 2025]

## **Awesome demo**

- [FRVR Official Teaserüì∫](https://youtu.be/Yjjpr-eAkqw): Prompt to Game: AI-powered end-to-end game creation [16 Jun 2023]
- [rewind.ai](https://www.rewind.ai/): Rewind captures everything you‚Äôve seen on your Mac and iPhone [Nov 2023]
- [Vercel announced V0.dev](https://v0.dev/chat/AjJVzgx): Make a snake game with chat [Oct 2023]
- [Mobile ALOHAüì∫](https://youtu.be/HaaZ8ss-HP4?si=iMYKzvx8wQhf39yU): A day of Mobile ALOHA [4 Jan 2024]
- [groq‚ú®](https://github.com/groq): An LPU Inference Engine, the LPU is reported to be 10 times faster than NVIDIA‚Äôs GPU performance [‚úçÔ∏è](https://www.gamingdeputy.com/groq-unveils-worlds-fastest-large-model-500-tokens-per-second-shatters-record-self-developed-lpu-outperforms-nvidia-gpu-by-10-times/) [Jan 2024]
- [Soraüì∫](https://youtu.be/HK6y8DAPN_0?si=FPZaGk4fP2d456QP): Introducing Sora ‚Äî OpenAI‚Äôs text-to-video model [Feb 2024]
- [Oasis‚úçÔ∏è](https://www.etched.com/blog-posts/oasis): Minecraft clone. Generated by AI in Real-Time. The first playable AI model that generates open-world games. [‚úçÔ∏è](https://oasis-model.github.io/) [‚ú®](https://github.com/etched-ai/open-oasis) [31 Oct 2024] ![**github stars**](https://img.shields.io/github/stars/etched-ai/open-oasis?style=flat-square&label=%20&color=blue&cacheSeconds=36000)

## **Datasets for LLM Training**

- LLM-generated datasets:
  - [Self-Instructüìë](https://arxiv.org/abs/2212.10560): Seed task pool with a set of human-written instructions. [20 Dec 2022]
  - [Self-Alignment with Instruction Backtranslationüìë](https://arxiv.org/abs/2308.06259): Without human seeding, use LLM to produce instruction-response pairs. The process involves two steps: self-augmentation and self-curation. [11 Aug 2023]
- [LLMDataHub: Awesome Datasets for LLM Training‚ú®](https://github.com/Zjh-819/LLMDataHub): A quick guide (especially) for trending instruction finetuning datasets
 ![**github stars**](https://img.shields.io/github/stars/Zjh-819/LLMDataHub?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
- [Open LLMs and Datasets‚ú®](https://github.com/eugeneyan/open-llms): A list of open LLMs available for commercial use.
 ![**github stars**](https://img.shields.io/github/stars/eugeneyan/open-llms?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
- [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/): The Stanford Question Answering Dataset (SQuAD), a set of Wikipedia articles, 100,000+ question-answer pairs on 500+ articles. [16 Jun 2016]
- [Synthetic Data Vault (SDV) ‚ú®](https://github.com/sdv-dev/SDV): Synthetic data generation for tabular data [May 2018] ![**github stars**](https://img.shields.io/github/stars/sdv-dev/SDV?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
- [RedPajama](https://together.ai/blog/redpajama): LLaMA training dataset of over 1.2 trillion tokens [‚ú®](https://github.com/togethercomputer/RedPajama-Data) [17 Apr 2023]
 ![**github stars**](https://img.shields.io/github/stars/togethercomputer/RedPajama-Data?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
- [FineWebü§ó](https://huggingface.co/datasets/HuggingFaceFW/fineweb):ü§óHuggingFace. crawled 15 trillion tokens of high-quality web data from the summer of 2013 to March 2024. [Apr 2024]
- [MS MARCO Web Search‚ú®](https://github.com/microsoft/MS-MARCO-Web-Search): A large-scale information-rich web dataset, featuring millions of real clicked query-document labels [Apr 2024]
 ![**github stars**](https://img.shields.io/github/stars/microsoft/MS-MARCO-Web-Search?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
- [Nemotron-Personas-Japan: Synthesized Data for Sovereign AIü§ó](https://huggingface.co/blog/nvidia/nemotron-personas-japan): The first open synthetic dataset that captures Japan's demographic, geographic, and cultural spectrum.  [23 Sep 2025]
- [Synthetic Data of LLMs‚ú®](https://github.com/wasiahmad/Awesome-LLM-Synthetic-Data): A reading list on LLM based Synthetic Data Generation [Oct 2024]
 ![**github stars**](https://img.shields.io/github/stars/wasiahmad/Awesome-LLM-Synthetic-Data?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
- [Open Thoughts‚ú®](https://github.com/open-thoughts/open-thoughts): Fully Open Data Curation for Thinking Models [28 Jan 2025] ![**github stars**](https://img.shields.io/github/stars/open-thoughts/open-thoughts?style=flat-square&label=%20&color=blue&cacheSeconds=36000)

Pretrain for a base model

```json
{
    "text": ...,
    "meta": {"url": "...", "timestamp": "...", "source": "...", "language": "...", ...},
    "red_pajama_subset": "common_crawl" | "c4" | "github" | "books" | "arxiv" | "wikipedia" | "stackexchange"
}
```

databricks-dolly-15k: Instruction-Tuned [‚ú®ü§ó](https://huggingface.co/datasets/databricks/databricks-dolly-15k): SFT training - QA pairs or Dialog

```json
{
  "prompt": "What is the capital of France?",
  "response": "The capital of France is Paris."
},
{
    "prompt": "Can you give me a recipe for chocolate chip cookies?",
    "response": "Sure! ..."
}
```

[Anthropic human-feedback‚ú®ü§ó](https://huggingface.co/datasets/Anthropic/hh-rlhf): RLHF training - Chosen and Rejected pairs

```json
{
  "chosen": "I'm sorry to hear that. Is there anything I can do to help?",
  "rejected": "That's too bad. You should just get over it."
}
```

<!-- - [Â§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åæ„Å®„ÇÅ](https://note.com/npaka/n/n686d987adfb1): Â§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åæ„Å®„ÇÅ [Apr 2023] -->
- Dataset example

  [üó£Ô∏è](https://docs.argilla.io/)

  ### SFT Dataset

  | Category | Instruction | Context | Response |
  | --- | --- | --- | --- |
  | 0 | Open QA | How do I get rid of mosquitos in my house? | You can get rid of mosquitos in your house by ... |
  | 1 | Classification | Classify each country as "African" or "European" | Nigeria: African<br>Rwanda: African<br>Portugal: European |
  | 2 | Information Extraction | Extract the unique names of composers from the text. | To some extent, European and the US traditions... Pierre Boulez, Luigi Nono, Karlheinz Stockhausen |
  | 3 | General QA | Should investors time the market? | Timing the market is based on predictions of t... |

  ### RLHF Dataset

  | Instruction | Chosen Response | Rejected Response |
  | --- | --- | --- |
  | What is Depreciation | Depreciation is the drop in value of an asset ... | What is Depreciation ‚Äì 10 Important Facts to K... |
  | What do you know about the city of Aberdeen in Scotland? | Aberdeen is a city located in the North East of Scotland. It is known for its granite architecture and its offshore oil industry. | As an AI language model, I don't have personal knowledge or experiences about Aberdeen. |
  | Describe thunderstorm season in the United States and Canada. | Thunderstorm season in the United States and Canada typically occurs during the spring and summer months, when warm, moist air collides with cooler, drier air, creating the conditions for thunderstorms to form. | Describe thunderstorm season in the United States and Canada. |

## **Evaluating Large Language Models**

- [Artificial Analysis LLM Performance Leaderboardü§ó](https://huggingface.co/spaces/ArtificialAnalysis/LLM-Performance-Leaderboard): Performance benchmarks & pricing across API providers of LLMs
- Awesome LLMs Evaluation Papers: Evaluating Large Language Models: A Comprehensive Survey [‚ú®](https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers) [Oct 2023]
 ![**github stars**](https://img.shields.io/github/stars/tjunlp-lab/Awesome-LLMs-Evaluation-Papers?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
- [Can Large Language Models Be an Alternative to Human Evaluations?üìë](https://arxiv.org/abs/2305.01937) [3 May 2023]
- [ChatGPT‚Äôs One-year Anniversary: Are Open-Source Large Language Models Catching up?üìë](https://arxiv.org/abs/2311.16989): Open-Source LLMs vs. ChatGPT; Benchmarks and Performance of LLMs [28 Nov 2023]
- Evaluation of Large Language Models: [A Survey on Evaluation of Large Language Modelsüìë](https://arxiv.org/abs/2307.03109): [6 Jul 2023]
- [Evaluation Papers for ChatGPT‚ú®](https://github.com/THU-KEG/EvaluationPapers4ChatGPT) [28 Feb 2023]
 ![**github stars**](https://img.shields.io/github/stars/THU-KEG/EvaluationPapers4ChatGPT?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
- [Evaluating the Effectiveness of LLM-Evaluators (aka LLM-as-Judge)](https://eugeneyan.com/writing/llm-evaluators/):üí°Key considerations and Use cases when using LLM-evaluators [Aug 2024]
- [LightEval‚ú®](https://github.com/huggingface/lighteval):ü§ó a lightweight LLM evaluation suite that Hugging Face has been using internally [Jan 2024]
 ![**github stars**](https://img.shields.io/github/stars/huggingface/lighteval?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
- [LLM Model Evals vs LLM Task Evals](https://x.com/aparnadhinak/status/1752763354320404488)
: `Model Evals` are really for people who are building or fine-tuning an LLM. vs The best LLM application builders are using `Task evals`. It's a tool to help builders build. [Feb 2024]
- [LLMPerf Leaderboard‚ú®](https://github.com/ray-project/llmperf-leaderboard): Evaulation the performance of LLM APIs. [Dec 2023]
 ![**github stars**](https://img.shields.io/github/stars/ray-project/llmperf-leaderboard?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
- [LLM-as-a-Judge](https://cameronrwolfe.substack.com/i/141159804/practical-takeaways):üí°LLM-as-a-Judge offers a quick, cost-effective way to develop models aligned with human preferences and is easy to implement with just a prompt, but should be complemented by human evaluation to address biases.  [Jul 2024]
- [OCR Arena](https://www.ocrarena.ai/battle): a free playground for testing and evaluating leading foundation VLMs and open source OCR models side-by-side. [Nov 2025]
- [Prometheus: Inducing Fine-grained Evaluation Capability in Language Modelsüìë](https://arxiv.org/abs/2310.08491): We utilize the FEEDBACK COLLECTION, a novel dataset, to train PROMETHEUS, an open-source large language model with 13 billion parameters, designed specifically for evaluation tasks. [12 Oct 2023]
- [The Leaderboard Illusionüìë](https://arxiv.org/abs/2504.20879):üí°Chatbot Arena's benchmarking is skewed by selective disclosures, private testing advantages, and data access asymmetries, leading to overfitting and unfair model rankings. [29 Apr 2025]

### **LLM Evalution Benchmarks**

#### Language Understanding and QA

1. [BIG-benchüìë](https://arxiv.org/abs/2206.04615): Consists of 204 evaluations, contributed by over 450 authors, that span a range of topics from science to social reasoning. The bottom-up approach; anyone can submit an evaluation task. [‚ú®](https://github.com/google/BIG-bench) [9 Jun 2022]
![**github stars**](https://img.shields.io/github/stars/google/BIG-bench?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [BigBench‚ú®](https://github.com/google/BIG-bench): 204 tasks. Predicting future potential [Published in 2023]
![**github stars**](https://img.shields.io/github/stars/google/BIG-bench?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [GLUE](https://gluebenchmark.com/leaderboard) & [SuperGLUE](https://super.gluebenchmark.com/leaderboard/): GLUE (General Language Understanding Evaluation)
1. [HELMüìë](https://arxiv.org/abs/2211.09110): Evaluation scenarios like reasoning and disinformation using standardized metrics like accuracy, calibration, robustness, and fairness. The top-down approach; experts curate and decide what tasks to evaluate models on. [‚ú®](https://github.com/stanford-crfm/helm) [16 Nov 2022] ![**github stars**](https://img.shields.io/github/stars/stanford-crfm/helm?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [HumanEvalüìë](https://arxiv.org/abs/2107.03374): Hand-Written Evaluation Set for Code Generation Bechmark. 164 Human written Programming Problems. [‚úçÔ∏è](https://paperswithcode.com/task/code-generation) / [‚ú®](https://github.com/openai/human-eval) [7 Jul 2021]
![**github stars**](https://img.shields.io/github/stars/openai/human-eval?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [MMLU (Massive Multitask Language Understanding)‚ú®](https://github.com/hendrycks/test): Over 15,000 questions across 57 diverse tasks. [Published in 2021]
![**github stars**](https://img.shields.io/github/stars/hendrycks/test?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [MMLU (Massive Multi-task Language Understanding)üìë](https://arxiv.org/abs/2009.03300): LLM performance across 57 tasks including elementary mathematics, US history, computer science, law, and more. [7 Sep 2020]
1. [TruthfulQAü§ó](https://huggingface.co/datasets/truthful_qa): Truthfulness. [Published in 2022]

#### Coding

1. [CodeXGLUE‚ú®](https://github.com/microsoft/CodeXGLUE): Programming tasks.
![**github stars**](https://img.shields.io/github/stars/microsoft/CodeXGLUE?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [HumanEval‚ú®](https://github.com/openai/human-eval): Challenges coding skills. [Published in 2021]
![**github stars**](https://img.shields.io/github/stars/openai/human-eval?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [MBPP‚ú®](https://github.com/google-research/google-research/tree/master/mbpp): Mostly Basic Python Programming. [Published in 2021]
1. [SWE-bench](https://www.swebench.com/): Software Engineering Benchmark. Real-world software issues sourced from GitHub.
1. [SWE-Lancer‚úçÔ∏è](https://openai.com/index/swe-lancer/): OpenAI. full engineering stack, from UI/UX to systems design, and include a range of task types, from $50 bug fixes to $32,000 feature implementations. [18 Feb 2025]
1. [Vibe Code Bench](https://www.vals.ai/benchmarks/vibe-code): Claude Sonnet 4.5 (Thinking)and GPT 5.1 are head and shoulders above the competition. GPT 5.1 stands out especially for its low cost and high performance.

#### Chatbot Assistance

1. [Chatbot Arenaü§ó](https://huggingface.co/datasets/lmsys/chatbot_arena_conversations): Human-ranked ELO ranking.
1. [MT Bench‚ú®](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge): Multi-turn open-ended questions
  - [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arenaüìë](https://arxiv.org/abs/2306.05685) [9 Jun 2023]

#### Reasoning

1. [ARC (AI2 Reasoning Challenge)‚ú®](https://github.com/fchollet/ARC): Measures general fluid intelligence.
![**github stars**](https://img.shields.io/github/stars/fchollet/ARC?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [DROPü§ó](https://huggingface.co/datasets/drop): Evaluates discrete reasoning.
1. [HellaSwag‚ú®](https://github.com/rowanz/hellaswag): Commonsense reasoning. [Published in 2019]
![**github stars**](https://img.shields.io/github/stars/rowanz/hellaswag?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [LogicQA‚ú®](https://github.com/lgw863/LogiQA-dataset): Evaluates logical reasoning skills.
![**github stars**](https://img.shields.io/github/stars/lgw863/LogiQA-dataset?style=flat-square&label=%20&color=blue&cacheSeconds=36000)

#### Translation

1. [WMTü§ó](https://huggingface.co/wmt): Evaluates translation skills.

#### Math

1. [GSM8K‚ú®](https://github.com/openai/grade-school-math): Arithmetic Reasoning. [Published in 2021]
![**github stars**](https://img.shields.io/github/stars/openai/grade-school-math?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [MATH‚ú®](https://github.com/hendrycks/math): Tests ability to solve math problems. [Published in 2021]
![**github stars**](https://img.shields.io/github/stars/hendrycks/math?style=flat-square&label=%20&color=blue&cacheSeconds=36000)

 #### Other Benchmarks

- [Alpha Arena](https://nof1.ai/): a benchmark designed to measure AI's investing abilities. [Oct 2025]
- [Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answeringüìë](https://arxiv.org/abs/2411.09213) [14 Nov 2024]
- [Korean SAT LLM Leaderboard‚ú®](https://github.com/Marker-Inc-Korea/Korean-SAT-LLM-Leaderboard): Benchmarking 10 years of Korean CSAT (College Scholastic Ability Test) exams [Oct 2024]
![**github stars**](https://img.shields.io/github/stars/Marker-Inc-Korea/Korean-SAT-LLM-Leaderboard?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
- [OpenAI BrowseComp‚úçÔ∏è](https://openai.com/index/browsecomp/): A benchmark assessing AI agents‚Äô ability to use web browsing tools to complete tasks requiring up-to-date information, reasoning, and navigation skills. Boost from tools + reasoning. Human trainer success ratio = 29.2% √ó 86.4% ‚âà 25.2% [10 Apr 2025]
- [OpenAI GDPval‚úçÔ∏è](https://openai.com/index/gdpval/): OpenAI's benchmark evaluating AI performance on real-world tasks across 44 occupations [25 Sep 2025]
- [OpenAI MLE-benchüìë](https://arxiv.org/abs/2410.07095): A benchmark for measuring the performance of AI agents on ML tasks using Kaggle. [‚ú®](https://github.com/openai/mle-bench) [9 Oct 2024] > Agent Framework used in MLE-bench, `GPT-4o (AIDE) achieves more medals on average than both MLAB and OpenHands (8.7% vs. 0.8% and 4.4% respectively)` 
![**github stars**](https://img.shields.io/github/stars/openai/mle-bench?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
- [OpenAI Paper Bench‚úçÔ∏è](https://openai.com/index/paperbench/): a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research. [‚ú®](https://github.com/openai/preparedness/tree/main/project/paperbench) [2 Apr 2025]
- [OpenAI SimpleQA Benchmark‚úçÔ∏è](https://openai.com/index/introducing-simpleqa/): SimpleQA, a factuality benchmark for short fact-seeking queries, narrows its scope to simplify factuality measurement. [‚ú®](https://github.com/openai/simple-evals) [30 Oct 2024] ![**github stars**](https://img.shields.io/github/stars/openai/simple-evals?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
- [Social Sycophancy: A Broader Understanding of LLM Sycophancyüìë](https://arxiv.org/abs/2505.13995): ELEPHANT; LLM Benchmark to assess LLM Sycophancy. Dataset (query): OEQ (Open-Ended Questions) and Reddit. LLMs (prompted as judges) to assess the presence of sycophancy in outputs with prompt [20 May¬†2025]

### **Evaluation Metrics**

- [Evaluating LLMs and RAG Systems‚úçÔ∏è](https://dzone.com/articles/evaluating-llms-and-rag-systems) (Jan 2025)
- Automated evaluation
  - **n-gram metrics**: ROUGE, BLEU, METEOR ‚Üí compare overlap with reference text.
  - *ROUGE*: multiple variants (N, L, W, S, SU) based on n-gram, LCS, skip-bigrams.
  - *BLEU*: 0‚Äì1 score for translation quality.
  - *METEOR*: precision + recall + semantic similarity.
  - **Probabilistic metrics**: *Perplexity* ‚Üí lower is better predictive performance.
  - **Embedding metrics**: Ada Similarity, BERTScore ‚Üí semantic similarity using embeddings.
- Human evaluation
    - Measures **relevance, fluency, coherence, groundedness**.
    - Automated with LLM-based evaluators.
- Built-in methods
    - Prompt flow evaluation methods: [‚úçÔ∏è](https://qiita.com/nohanaga/items/b68bf5a65142c5af7969) [Aug 2023] / [‚úçÔ∏è](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/how-to-bulk-test-evaluate-flow)

## **LLMOps: Large Language Model Operations**

1. [agenta‚ú®](https://github.com/Agenta-AI/agenta): OSS LLMOps workflow: building (LLM playground, evaluation), deploying (prompt and configuration management), and monitoring (LLM observability and tracing). [Jun 2023] ![**github stars**](https://img.shields.io/github/stars/Agenta-AI/agenta?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [Azure ML Prompt flow](https://microsoft.github.io/promptflow/index.html): A set of LLMOps tools designed to facilitate the creation of LLM-based AI applications [Sep 2023] > [How to Evaluate & Upgrade Model Versions in the Azure OpenAI Service‚úçÔ∏è](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/how-to-evaluate-amp-upgrade-model-versions-in-the-azure-openai/ba-p/4218880) [14 Aug 2024]
1. Azure Machine Learning studio Model Data Collector: Collect production data, analyze key safety and quality evaluation metrics on a recurring basis, receive timely alerts about critical issues, and visualize the results. [‚úçÔ∏è](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-collect-production-data?view=azureml-api-2&tabs=azure-cli) [Apr 2024]
1. [circuit‚Äëtracer‚ú®](https://github.com/safety-research/circuit-tracer): Anthrophic. Tool for finding and visualizing circuits within large language models. a circuit is a minimal, causal computation pathway inside a transformer model that shows how internal features lead to a specific output. [May 2025] ![**github stars**](https://img.shields.io/github/stars/safety-research/circuit-tracer?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [DeepEval‚ú®](https://github.com/confident-ai/deepeval): LLM evaluation framework. similar to Pytest but specialized for unit testing LLM outputs. [Aug 2023]
 ![**github stars**](https://img.shields.io/github/stars/confident-ai/deepeval?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [DeepTeam‚ú®](https://github.com/confident-ai/deepteam): A LLM Red Teaming Framework. [Mar 2025] ![**github stars**](https://img.shields.io/github/stars/confident-ai/deepteam?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [promptfoo‚ú®](https://github.com/promptfoo/promptfoo): LLM red teaming and evaluation framework for testing jailbreaks, prompt injection, and vulnerabilities. CI/CD integration. [Apr 2023] ![**github stars**](https://img.shields.io/github/stars/promptfoo/promptfoo?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [Giskard‚ú®](https://github.com/Giskard-AI/giskard): The testing framework for ML models, from tabular to LLMs [Mar 2022] ![**github stars**](https://img.shields.io/github/stars/Giskard-AI/giskard?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [Langfuse](https://langfuse.com): [‚ú®](https://github.com/langfuse/langfuse) LLMOps platform that helps teams to collaboratively monitor, evaluate and debug AI applications. [May 2023] 
 ![**github stars**](https://img.shields.io/github/stars/langfuse/langfuse?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [Language Model Evaluation Harness‚ú®](https://github.com/EleutherAI/lm-evaluation-harness):üí°Over 60 standard academic benchmarks for LLMs. A framework for few-shot evaluation. Hugginface uses this for [Open LLM Leaderboardü§ó](https://huggingface.co/open-llm-leaderboard) [Aug 2020]
 ![**github stars**](https://img.shields.io/github/stars/EleutherAI/lm-evaluation-harness?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [LangWatch scenario‚ú®](https://github.com/langwatch/scenario):üí°LangWatch Agentic testing for agentic codebases. Simulating agentic communication using autopilot [Apr 2025] ![**github stars**](https://img.shields.io/github/stars/langwatch/scenario?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [LLMOps Database](https://www.zenml.io/llmops-database): A curated knowledge base of real-world LLMOps implementations.
1. [Maxim AI](https://getmaxim.ai): [‚ú®](https://github.com/maximhq) End-to-end simulation, evaluation, and observability plaform, helping teams ship their AI agents reliably and >5x faster. [Dec 2023]
1. [Machine Learning Operations (MLOps) For Beginners‚úçÔ∏è](https://towardsdatascience.com/machine-learning-operations-mlops-for-beginners-a5686bfe02b2): DVC (Data Version Control), MLflow, Evidently AI (Monitor a model). Insurance Cross Sell Prediction [‚ú®](https://github.com/prsdm/mlops-project) [29 Aug 2024]
 ![**github stars**](https://img.shields.io/github/stars/prsdm/mlops-project?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [Netdata‚ú®](https://github.com/netdata/netdata): AI-powered real-time infrastructure monitoring platform [Jun 2013] ![**github stars**](https://img.shields.io/github/stars/netdata/netdata?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [OpenAI Evals‚ú®](https://github.com/openai/evals): A framework for evaluating large language models (LLMs) [Mar 2023]
 ![**github stars**](https://img.shields.io/github/stars/openai/evals?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [Opik‚ú®](https://github.com/comet-ml/opik): an open-source platform for evaluating, testing and monitoring LLM applications. Built by Comet. [2 Sep 2024] ![**github stars**](https://img.shields.io/github/stars/comet-ml/opik?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [Pezzo‚ú®](https://github.com/pezzolabs/pezzo): Open-source, developer-first LLMOps platform [May 2023]
 ![**github stars**](https://img.shields.io/github/stars/pezzolabs/pezzo?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [promptfoo‚ú®](https://github.com/promptfoo/promptfoo): Test your prompts. Evaluate and compare LLM outputs, catch regressions, and improve prompt quality. [Apr 2023]
 ![**github stars**](https://img.shields.io/github/stars/promptfoo/promptfoo?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [PromptTools‚ú®](https://github.com/hegelai/prompttools/): Open-source tools for prompt testing [Jun 2023] ![**github stars**](https://img.shields.io/github/stars/hegelai/prompttools?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [Ragas‚ú®](https://github.com/explodinggradients/ragas): Evaluation framework for your Retrieval Augmented Generation (RAG) [May 2023]
 ![**github stars**](https://img.shields.io/github/stars/explodinggradients/ragas?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [traceloop openllmetry‚ú®](https://github.com/traceloop/openllmetry): Quality monitoring for your LLM applications. [Sep 2023]
 ![**github stars**](https://img.shields.io/github/stars/traceloop/openllmetry?style=flat-square&label=%20&color=blue&cacheSeconds=36000)
1. [TruLens‚ú®](https://github.com/truera/trulens): Instrumentation and evaluation tools for large language model (LLM) based applications. [Nov 2020]
 ![**github stars**](https://img.shields.io/github/stars/truera/trulens?style=flat-square&label=%20&color=blue&cacheSeconds=36000)

### **Challenges in evaluating AI systems**

1. [30 requirements for an MLOps environmentüó£Ô∏è](https://x.com/KirkDBorne/status/1679952405805555713): Kirk Borne twitter [15 Jul 2023]
1. [Challenges in evaluating AI systems‚úçÔ∏è](https://www.anthropic.com/index/evaluating-ai-systems): The challenges and limitations of various methods for evaluating AI systems, such as multiple-choice tests, human evaluations, red teaming, model-generated evaluations, and third-party audits. [üóÑÔ∏è](../files/eval-ai-anthropic.pdf) [4 Oct 2023]
1. [Economics of Hosting Open Source LLMs‚úçÔ∏è](https://towardsdatascience.com/economics-of-hosting-open-source-llms-17b4ec4e7691): Comparison of cloud vendors such as AWS, Modal, BentoML, Replicate, Hugging Face Endpoints, and Beam, using metrics like processing time, cold start latency, and costs associated with CPU, memory, and GPU usage. [‚ú®](https://github.com/ilsilfverskiold/Awesome-LLM-Resources-List) [13 Nov 2024]
1. [Pretraining on the Test Set Is All You Needüìë](https://arxiv.org/abs/2309.08632): On that note, in the satirical¬†Pretraining on the Test Set Is All You Need¬†paper, the author trains a small 1M parameter LLM that outperforms all other models, including the 1.3B phi-1.5 model. This is achieved by training the model on all downstream academic benchmarks. It appears to be a subtle criticism underlining how easily benchmarks can be "cheated" intentionally or unintentionally (due to data contamination). [üó£Ô∏è](https://twitter.com/rasbt) [13 Sep 2023]
1. [Sakana AI claimed 100x faster AI training, but a bug caused a 3x slowdown](https://techcrunch.com/2025/02/21/sakana-walks-back-claims-that-its-ai-can-dramatically-speed-up-model-training/): Sakana‚Äôs AI resulted in a 3x slowdown ‚Äî not a speedup. [21 Feb 2025]
1. [Your AI Product Needs Evals](https://hamel.dev/blog/posts/evals/) [29 Mar 2024] / [How to Evaluate LLM Applications: The Complete Guide](https://www.confident-ai.com/blog/how-to-evaluate-llm-applications) [7 Nov 2023]
